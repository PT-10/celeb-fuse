{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12536282,"sourceType":"datasetVersion","datasetId":7914215},{"sourceId":12588026,"sourceType":"datasetVersion","datasetId":7950321},{"sourceId":485623,"sourceType":"modelInstanceVersion","modelInstanceId":387555,"modelId":406607}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.datasets import ImageFolder\n\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torch.optim as optim","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T16:15:14.825887Z","iopub.execute_input":"2025-07-27T16:15:14.826127Z","iopub.status.idle":"2025-07-27T16:15:24.635395Z","shell.execute_reply.started":"2025-07-27T16:15:14.826105Z","shell.execute_reply":"2025-07-27T16:15:24.634669Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"image_dir = \"/kaggle/input/celeba-attribute/img_align_celeba/img_align_celeba\"\nattr_path = \"/kaggle/input/celeba-attribute/list_attr_celeba.txt\"\nsplit_path = \"/kaggle/input/celeba-attribute/list_eval_partition.txt\"\n\n# Transforms for ResNet\ntransform = transforms.Compose([\n    transforms.CenterCrop(178),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                     std=[0.229, 0.224, 0.225])\n    # transforms.Normalize([0.5]*3, [0.5]*3)\n])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-27T16:15:24.637106Z","iopub.execute_input":"2025-07-27T16:15:24.637637Z","iopub.status.idle":"2025-07-27T16:15:24.642092Z","shell.execute_reply.started":"2025-07-27T16:15:24.637617Z","shell.execute_reply":"2025-07-27T16:15:24.641370Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"attr_df = pd.read_csv(attr_path, sep='\\s+', skiprows=1)\n# Make the index a column\nattr_df.reset_index(inplace=True)\nattr_df.rename(columns={\"index\": \"image_id\"}, inplace=True)\n\n# Map -1 to 0 (for binary classification)\nattr_df.replace(-1, 0, inplace=True)\n\n# Load evaluation split\neval_df = pd.read_csv(\"/kaggle/input/celeba-attribute/list_eval_partition.txt\", \n                      sep='\\s+', header=None, names=[\"image_id\", \"split\"])\nattr_df = attr_df.merge(eval_df, on=\"image_id\")\nattr_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T16:15:24.642767Z","iopub.execute_input":"2025-07-27T16:15:24.642983Z","iopub.status.idle":"2025-07-27T16:15:26.178315Z","shell.execute_reply.started":"2025-07-27T16:15:24.642966Z","shell.execute_reply":"2025-07-27T16:15:26.177513Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"          image_id  5_o_Clock_Shadow  Arched_Eyebrows  Attractive  \\\n0       000001.jpg                 0                1           1   \n1       000002.jpg                 0                0           0   \n2       000003.jpg                 0                0           0   \n3       000004.jpg                 0                0           1   \n4       000005.jpg                 0                1           1   \n...            ...               ...              ...         ...   \n202594  202595.jpg                 0                0           1   \n202595  202596.jpg                 0                0           0   \n202596  202597.jpg                 0                0           0   \n202597  202598.jpg                 0                1           1   \n202598  202599.jpg                 0                1           1   \n\n        Bags_Under_Eyes  Bald  Bangs  Big_Lips  Big_Nose  Black_Hair  ...  \\\n0                     0     0      0         0         0           0  ...   \n1                     1     0      0         0         1           0  ...   \n2                     0     0      0         1         0           0  ...   \n3                     0     0      0         0         0           0  ...   \n4                     0     0      0         1         0           0  ...   \n...                 ...   ...    ...       ...       ...         ...  ...   \n202594                0     0      0         1         0           0  ...   \n202595                0     0      1         1         0           0  ...   \n202596                0     0      0         0         0           1  ...   \n202597                0     0      0         1         0           1  ...   \n202598                0     0      0         0         0           0  ...   \n\n        Smiling  Straight_Hair  Wavy_Hair  Wearing_Earrings  Wearing_Hat  \\\n0             1              1          0                 1            0   \n1             1              0          0                 0            0   \n2             0              0          1                 0            0   \n3             0              1          0                 1            0   \n4             0              0          0                 0            0   \n...         ...            ...        ...               ...          ...   \n202594        0              0          0                 0            0   \n202595        1              1          0                 0            0   \n202596        1              0          0                 0            0   \n202597        1              0          1                 1            0   \n202598        0              0          1                 0            0   \n\n        Wearing_Lipstick  Wearing_Necklace  Wearing_Necktie  Young  split  \n0                      1                 0                0      1      0  \n1                      0                 0                0      1      0  \n2                      0                 0                0      1      0  \n3                      1                 1                0      1      0  \n4                      1                 0                0      1      0  \n...                  ...               ...              ...    ...    ...  \n202594                 1                 0                0      1      2  \n202595                 0                 0                0      1      2  \n202596                 0                 0                0      1      2  \n202597                 1                 0                0      1      2  \n202598                 1                 0                0      1      2  \n\n[202599 rows x 42 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>5_o_Clock_Shadow</th>\n      <th>Arched_Eyebrows</th>\n      <th>Attractive</th>\n      <th>Bags_Under_Eyes</th>\n      <th>Bald</th>\n      <th>Bangs</th>\n      <th>Big_Lips</th>\n      <th>Big_Nose</th>\n      <th>Black_Hair</th>\n      <th>...</th>\n      <th>Smiling</th>\n      <th>Straight_Hair</th>\n      <th>Wavy_Hair</th>\n      <th>Wearing_Earrings</th>\n      <th>Wearing_Hat</th>\n      <th>Wearing_Lipstick</th>\n      <th>Wearing_Necklace</th>\n      <th>Wearing_Necktie</th>\n      <th>Young</th>\n      <th>split</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000001.jpg</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000002.jpg</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000003.jpg</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>000004.jpg</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>000005.jpg</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>202594</th>\n      <td>202595.jpg</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>202595</th>\n      <td>202596.jpg</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>202596</th>\n      <td>202597.jpg</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>202597</th>\n      <td>202598.jpg</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>202598</th>\n      <td>202599.jpg</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>202599 rows × 42 columns</p>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"class CelebABinaryHairDataset(Dataset):\n    def __init__(self, df, image_root, transform=None):\n        self.df = df.reset_index(drop=True)\n        self.image_root = image_root\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_path = os.path.join(self.image_root, row['image_id'])\n        image = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n        label = torch.tensor(row['Blond_Hair'], dtype=torch.float32)\n        gender = torch.tensor(row['Male'], dtype=torch.int64)\n        return image, label, gender","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T16:15:26.179047Z","iopub.execute_input":"2025-07-27T16:15:26.179267Z","iopub.status.idle":"2025-07-27T16:15:26.184853Z","shell.execute_reply.started":"2025-07-27T16:15:26.179251Z","shell.execute_reply":"2025-07-27T16:15:26.184135Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class SyntheticHairDataset(Dataset):\n    def __init__(self, image_dir, transform=None):\n        self.image_paths = []\n        self.labels = []\n        self.genders = []\n        self.transform = transform\n\n        subgroup_map = {\n            \"generated_images_male_blond\": (1, 1),\n            \"generated_female_blond\": (1, 0),\n            \"generated_images_male_non_blond\": (0, 1),\n            \"generated_female_non_blond\": (0, 0)\n        }\n\n        for subgroup in os.listdir(image_dir):\n            subgroup_path = os.path.join(image_dir, subgroup)\n            if not os.path.isdir(subgroup_path) or subgroup not in subgroup_map:\n                continue\n\n            label, gender = subgroup_map[subgroup]\n\n            for img_name in os.listdir(subgroup_path):\n                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n                    self.image_paths.append(os.path.join(subgroup_path, img_name))\n                    self.labels.append(label)\n                    self.genders.append(gender)\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n        label = self.labels[idx]\n        gender = self.genders[idx]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label, gender","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T16:15:26.185665Z","iopub.execute_input":"2025-07-27T16:15:26.185868Z","iopub.status.idle":"2025-07-27T16:15:26.200391Z","shell.execute_reply.started":"2025-07-27T16:15:26.185851Z","shell.execute_reply":"2025-07-27T16:15:26.199734Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"train_df = attr_df[attr_df['split'] == 0]\nval_df = attr_df[attr_df['split'] == 1]\ntest_df  = attr_df[attr_df['split'] == 2]\n\n# Dataset instances\ntrain_set = CelebABinaryHairDataset(train_df, image_dir, transform)\nval_set = CelebABinaryHairDataset(val_df, image_dir, transform)\nsynthetic_dataset = SyntheticHairDataset(\"/kaggle/input/final-new/FINAL\", transform)\ntest_set  = CelebABinaryHairDataset(test_df, image_dir, transform)\n\n# Dataloaders\ntrain_loader = DataLoader(train_set, batch_size=64, shuffle=True, num_workers=0)\nval_loader = DataLoader(val_set, batch_size=64, shuffle=True, num_workers=0)\nsynthetic_loader = DataLoader(synthetic_dataset, batch_size=32, shuffle=True, num_workers=0)\ntest_loader  = DataLoader(test_set, batch_size=32, shuffle=False, num_workers=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T16:15:26.201014Z","iopub.execute_input":"2025-07-27T16:15:26.201177Z","iopub.status.idle":"2025-07-27T16:15:26.532414Z","shell.execute_reply.started":"2025-07-27T16:15:26.201164Z","shell.execute_reply":"2025-07-27T16:15:26.531745Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Training function with tqdm\ndef train_epoch_with_val(model, dataloader, criterion, optimizer, device='cuda'):\n    model.train()\n    running_loss = 0.0\n\n    for images, labels, gender in tqdm(dataloader, desc=\"Training\", leave=False):\n        images = images.to(device)\n        labels = labels.long().to(device)  # required for CrossEntropyLoss\n    \n        optimizer.zero_grad()\n        outputs = model(images)  # shape: (B, 2)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n    \n        running_loss += loss.item() * images.size(0)\n\n    epoch_loss = running_loss / len(dataloader.dataset)\n    return epoch_loss\n\n\ndef train_epoch(model, dataloader, criterion, optimizer, device='cuda'):\n    model.train()\n    running_loss = 0.0\n\n    for images, labels, genders in tqdm(dataloader, desc=\"Training\", leave=False):\n        images = images.to(device)\n        labels = labels.long().to(device)\n\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * images.size(0)\n\n    return running_loss / len(dataloader.dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T16:15:26.534358Z","iopub.execute_input":"2025-07-27T16:15:26.534592Z","iopub.status.idle":"2025-07-27T16:15:26.541718Z","shell.execute_reply.started":"2025-07-27T16:15:26.534574Z","shell.execute_reply":"2025-07-27T16:15:26.540971Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def evaluate_groupwise(model, dataloader, device='cuda'):\n    model.eval()\n    group_correct = {\n        \"Blond Male\": 0,\n        \"Blond Female\": 0,\n        \"Non-Blond Male\": 0,\n        \"Non-Blond Female\": 0\n    }\n    group_total = {k: 0 for k in group_correct}\n\n    with torch.no_grad():\n        for images, labels, genders in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            preds = torch.argmax(outputs, dim=1).cpu()\n            labels = labels.cpu()\n            genders = genders.cpu()\n\n            for pred, label, gender in zip(preds, labels, genders):\n                group = (\"Blond \" if label == 1 else \"Non-Blond \") + (\"Male\" if gender == 1 else \"Female\")\n                group_total[group] += 1\n                if pred == label:\n                    group_correct[group] += 1\n\n    for group in group_correct:\n        correct = group_correct[group]\n        total = group_total[group]\n        acc = 100 * correct / max(1, total)\n        print(f\"{group}: Accuracy = {acc:.2f}% ({correct}/{total})\")\n\n    overall_acc = 100 * sum(group_correct.values()) / max(1, sum(group_total.values()))\n    print(f\"\\nOverall Accuracy = {overall_acc:.2f}%\")\n    return overall_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T16:15:26.542425Z","iopub.execute_input":"2025-07-27T16:15:26.542646Z","iopub.status.idle":"2025-07-27T16:15:26.556633Z","shell.execute_reply.started":"2025-07-27T16:15:26.542629Z","shell.execute_reply":"2025-07-27T16:15:26.555894Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Model\nmodel = models.resnet18(pretrained=True)\nmodel.fc = nn.Linear(model.fc.in_features, 2)\nmodel = model.to('cuda')\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T15:44:50.586669Z","iopub.execute_input":"2025-07-27T15:44:50.587390Z","iopub.status.idle":"2025-07-27T15:44:50.787948Z","shell.execute_reply.started":"2025-07-27T15:44:50.587364Z","shell.execute_reply":"2025-07-27T15:44:50.787385Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"num_epochs = 5\nfor epoch in range(num_epochs):\n    train_loss = train_epoch(model, train_loader, criterion, optimizer, device=device)\n    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}\")\n\n# Save the final model\ntorch.save(model.state_dict(), \"resnet18_blond_classifier_synthetic_sampled_final.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\nfrom tqdm import tqdm\nimport torchvision.models as models\n\nmodel_test = models.resnet18()\nmodel_test.fc = nn.Linear(model_test.fc.in_features, 2)  # 2 outputs for CE\n\ncheckpoint_path = \"resnet18_blond_classifier_synthetic_sampled_final.pth\"\nmodel_test.load_state_dict(torch.load(checkpoint_path, map_location='cuda'))\n\nmodel_test = model_test.to('cuda')\n\nevaluate_groupwise(model_test, test_loader, device='cuda')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T06:29:52.231216Z","iopub.status.idle":"2025-07-27T06:29:52.231431Z","shell.execute_reply.started":"2025-07-27T06:29:52.231328Z","shell.execute_reply":"2025-07-27T06:29:52.231341Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Drift Visualization","metadata":{}},{"cell_type":"markdown","source":"### Embeddings","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Load pretrained feature extractor (e.g., ResNet18 without final FC layer)\nresnet = models.resnet18(pretrained=True)\nresnet.fc = torch.nn.Identity()  # remove final classification layer\nresnet.eval().cuda()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader = DataLoader(train_set, batch_size=64, shuffle=True, num_workers=0)\nsynthetic_loader = DataLoader(synthetic_dataset, batch_size=64, shuffle=True, num_workers=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T06:43:15.407066Z","iopub.execute_input":"2025-07-27T06:43:15.407381Z","iopub.status.idle":"2025-07-27T06:43:15.411570Z","shell.execute_reply.started":"2025-07-27T06:43:15.407344Z","shell.execute_reply":"2025-07-27T06:43:15.410786Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_embeddings(model, dataloader, max_samples=2000):\n    features, labels, genders = [], [], []\n    count = 0\n\n    with torch.no_grad():\n        for imgs, lbls, gdrs in dataloader:\n            imgs = imgs.cuda()\n            emb = model(imgs).cpu().numpy()\n            features.append(emb)\n            labels.extend(lbls.numpy())\n            genders.extend(gdrs.numpy())\n            count += len(imgs)\n            if count >= max_samples:\n                break\n\n    return np.concatenate(features), np.array(labels), np.array(genders)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T06:32:58.944809Z","iopub.execute_input":"2025-07-27T06:32:58.945324Z","iopub.status.idle":"2025-07-27T06:32:58.950162Z","shell.execute_reply.started":"2025-07-27T06:32:58.945302Z","shell.execute_reply":"2025-07-27T06:32:58.949541Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"real_feats, real_lbls, real_genders = get_embeddings(resnet, train_loader)\nsynth_feats, synth_lbls, synth_genders = get_embeddings(resnet, synthetic_loader)\n\nX = np.vstack([real_feats, synth_feats])\ny = np.concatenate([real_lbls, synth_lbls])\ng = np.concatenate([real_genders, synth_genders])\ndomain = ['real'] * len(real_feats) + ['synthetic'] * len(synth_feats)\n\n# Run t-SNE\ntsne = TSNE(n_components=2, perplexity=30, random_state=42)\nX_2d = tsne.fit_transform(X)\n\n# Plot\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x=X_2d[:, 0], y=X_2d[:, 1], hue=domain, style=g.astype(int), alpha=0.7)\nplt.title(\"t-SNE: Real vs Synthetic - Gender Style Overlay\")\nplt.legend(title=\"Domain / Gender\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T06:43:20.826354Z","iopub.execute_input":"2025-07-27T06:43:20.826622Z","iopub.status.idle":"2025-07-27T06:44:37.450615Z","shell.execute_reply.started":"2025-07-27T06:43:20.826603Z","shell.execute_reply":"2025-07-27T06:44:37.449886Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\ndf = pd.DataFrame({\n    'x': X_2d[:, 0],\n    'y': X_2d[:, 1],\n    'domain': domain,\n    'gender': g,\n    'label': y\n})\n\n# Map subgroup combinations to readable names\nsubgroup_map = {\n    (0, 0): 'Non-Blond Female',\n    (0, 1): 'Non-Blond Male',\n    (1, 0): 'Blond Female',\n    (1, 1): 'Blond Male'\n}\ndf['subgroup'] = df.apply(lambda row: subgroup_map[(row['label'], row['gender'])], axis=1)\n\n# --- Plot 1: All Samples Colored by Subgroup (Hair + Gender) ---\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=df, x='x', y='y', hue='subgroup', style='domain', alpha=0.7)\nplt.title(\"Plot 1: All Samples - Hair + Gender\")\nplt.legend(title=\"Subgroup\")\nplt.show()\n\n# --- Plot 2: Real Only ---\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=df[df['domain'] == 'real'], x='x', y='y', hue='subgroup', alpha=0.7)\nplt.title(\"Plot 2: Real Only - Hair + Gender\")\nplt.legend(title=\"Subgroup\")\nplt.show()\n\n# --- Plot 3: Synthetic Only ---\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=df[df['domain'] == 'synthetic'], x='x', y='y', hue='subgroup', alpha=0.7)\nplt.title(\"Plot 3: Synthetic Only - Hair + Gender\")\nplt.legend(title=\"Subgroup\")\nplt.show()\n\n# --- Plots 4–7: Real vs Synthetic for each Subgroup ---\nfor i, ((label_val, gender_val), name) in enumerate(subgroup_map.items(), start=4):\n    subset = df[(df['label'] == label_val) & (df['gender'] == gender_val)]\n    plt.figure(figsize=(8, 5))\n    sns.scatterplot(data=subset, x='x', y='y', hue='domain', alpha=0.7)\n    plt.title(f\"Plot {i}: Real vs Synthetic - {name}\")\n    plt.legend(title=\"Domain\")\n    plt.show()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T06:53:05.981782Z","iopub.execute_input":"2025-07-27T06:53:05.982523Z","iopub.status.idle":"2025-07-27T06:53:07.989290Z","shell.execute_reply.started":"2025-07-27T06:53:05.982497Z","shell.execute_reply":"2025-07-27T06:53:07.988510Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## DANN","metadata":{}},{"cell_type":"code","source":"from torch.autograd import Function\n\nclass GradReverse(Function):\n    @staticmethod\n    def forward(ctx, x, lambd):\n        ctx.lambd = lambd\n        return x.view_as(x)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return -ctx.lambd * grad_output, None\n\ndef grad_reverse(x, lambd=1.0):\n    return GradReverse.apply(x, lambd)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T16:16:54.992740Z","iopub.execute_input":"2025-07-27T16:16:54.993094Z","iopub.status.idle":"2025-07-27T16:16:54.998233Z","shell.execute_reply.started":"2025-07-27T16:16:54.993071Z","shell.execute_reply":"2025-07-27T16:16:54.997443Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import torch.nn as nn\nimport torchvision.models as models\n\nclass DANN(nn.Module):\n    def __init__(self, feature_dim=512, num_classes=2):\n        super().__init__()\n        # Load pretrained resnet backbone\n        self.backbone = models.resnet18(pretrained=False)\n        self.backbone.fc = nn.Identity()  # Remove final FC\n        self.feature_dim = feature_dim\n\n        # Classification head\n        self.classifier = nn.Linear(feature_dim, num_classes)\n\n        # Domain discriminator\n        self.domain_classifier = nn.Sequential(\n            nn.Linear(feature_dim, 100),\n            nn.ReLU(),\n            nn.Linear(100, 2)  # 2 domains: synthetic, real\n        )\n\n    def forward(self, x, lambd=0.0):\n        features = self.backbone(x)\n        class_logits = self.classifier(features)\n        reversed_features = grad_reverse(features, lambd)\n        domain_logits = self.domain_classifier(reversed_features)\n        return class_logits, domain_logits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T16:16:56.589067Z","iopub.execute_input":"2025-07-27T16:16:56.589329Z","iopub.status.idle":"2025-07-27T16:16:56.595276Z","shell.execute_reply.started":"2025-07-27T16:16:56.589310Z","shell.execute_reply":"2025-07-27T16:16:56.594659Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from itertools import cycle\nfrom tqdm import tqdm\nimport torch.nn.functional as F\nimport gc\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\npretrained = models.resnet18()\npretrained.fc = nn.Linear(pretrained.fc.in_features, 2)\npretrained.load_state_dict(torch.load(\"/kaggle/input/resnet18_blond_classifier_synthetic/pytorch/default/1/resnet18_blond_classifier_synthetic2.pth\", map_location='cpu'))\n\nmodel = DANN()\n\n# Load backbone weights (everything except fc)\ndann_backbone_dict = model.backbone.state_dict()\npretrained_backbone_dict = {\n    k: v for k, v in pretrained.state_dict().items()\n    if k in dann_backbone_dict and not k.startswith('fc')\n}\nmodel.backbone.load_state_dict(pretrained_backbone_dict)\n\n# Load classifier weights\nmodel.classifier.load_state_dict({\n    \"weight\": pretrained.state_dict()[\"fc.weight\"],\n    \"bias\": pretrained.state_dict()[\"fc.bias\"]\n})\n\nmodel = model.to('cuda')\n\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\ncriterion_cls = nn.CrossEntropyLoss()\ncriterion_domain = nn.CrossEntropyLoss()\n\nnum_epochs = 5\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss, total_cls_loss, total_domain_loss = 0, 0, 0\n    num_batches = 0\n\n    target_iter = cycle(test_loader)\n    progress_bar = tqdm(synthetic_loader, desc=f\"Epoch {epoch+1}\")\n\n    for (xs, ys, _), (xt, _, _) in zip(progress_bar, target_iter):\n        xs, ys = xs.to(device), ys.long().to(device)\n        xt = xt.to(device)\n\n        domain_s = torch.zeros(xs.size(0)).long().to(device)\n        domain_t = torch.ones(xt.size(0)).long().to(device)\n\n        x = torch.cat([xs, xt], dim=0)\n        class_logits, domain_logits = model(x, lambd=1.0)\n\n        class_preds = class_logits[:xs.size(0)]\n        domain_preds = domain_logits\n\n        loss_cls = criterion_cls(class_preds, ys)\n        loss_domain = criterion_domain(domain_preds, torch.cat([domain_s, domain_t]))\n        loss = loss_cls + 0.1 * loss_domain\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        total_cls_loss += loss_cls.item()\n        total_domain_loss += loss_domain.item()\n        num_batches += 1\n\n        # Optionally update tqdm live bar\n        progress_bar.set_postfix({\n            \"Last Total Loss\": f\"{loss.item():.2f}\",\n            \"Last Cls Loss\": f\"{loss_cls.item():.2f}\",\n            \"Last Domain Loss\": f\"{loss_domain.item():.2f}\"\n        })\n\n    # Epoch-wise averaged loss\n    avg_total_loss = total_loss / num_batches\n    avg_cls_loss = total_cls_loss / num_batches\n    avg_domain_loss = total_domain_loss / num_batches\n\n    print(f\"[Epoch {epoch+1}] Avg Total Loss: {avg_total_loss:.4f}, \"\n          f\"Cls Loss: {avg_cls_loss:.4f}, Domain Loss: {avg_domain_loss:.4f}\")\n\n    # Clear memory between epochs\n    torch.cuda.empty_cache()\n    gc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T16:17:22.408020Z","iopub.execute_input":"2025-07-27T16:17:22.408283Z","iopub.status.idle":"2025-07-27T17:00:01.769149Z","shell.execute_reply.started":"2025-07-27T16:17:22.408264Z","shell.execute_reply":"2025-07-27T17:00:01.768372Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\nEpoch 1: 100%|██████████| 619/619 [11:58<00:00,  1.16s/it, Last Total Loss=0.30, Last Cls Loss=0.25, Last Domain Loss=0.58]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 1] Avg Total Loss: 0.2892, Cls Loss: 0.2242, Domain Loss: 0.6493\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 619/619 [07:40<00:00,  1.35it/s, Last Total Loss=0.17, Last Cls Loss=0.07, Last Domain Loss=1.08]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 2] Avg Total Loss: 0.1959, Cls Loss: 0.1236, Domain Loss: 0.7227\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 619/619 [07:35<00:00,  1.36it/s, Last Total Loss=0.10, Last Cls Loss=0.03, Last Domain Loss=0.72]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 3] Avg Total Loss: 0.1475, Cls Loss: 0.0741, Domain Loss: 0.7333\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 619/619 [07:40<00:00,  1.34it/s, Last Total Loss=0.09, Last Cls Loss=0.02, Last Domain Loss=0.65]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 4] Avg Total Loss: 0.1174, Cls Loss: 0.0484, Domain Loss: 0.6897\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|██████████| 619/619 [07:42<00:00,  1.34it/s, Last Total Loss=0.09, Last Cls Loss=0.02, Last Domain Loss=0.64]","output_type":"stream"},{"name":"stdout","text":"[Epoch 5] Avg Total Loss: 0.1118, Cls Loss: 0.0406, Domain Loss: 0.7115\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch\n\ndef evaluate_groupwise(model, dataloader, device='cuda'):\n    model.eval()\n    group_correct = {\n        \"Blond Male\": 0,\n        \"Blond Female\": 0,\n        \"Non-Blond Male\": 0,\n        \"Non-Blond Female\": 0\n    }\n    group_total = {k: 0 for k in group_correct}\n\n    with torch.no_grad():\n        for images, labels, genders in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n            images = images.to(device)\n            labels = labels.to(device)\n            genders = genders.to(device)\n\n            class_logits, _ = model(images)\n            preds = torch.argmax(class_logits, dim=1)\n\n            labels = labels.cpu()\n            genders = genders.cpu()\n            preds = preds.cpu()\n\n            for pred, label, gender in zip(preds, labels, genders):\n                group = (\"Blond \" if label == 1 else \"Non-Blond \") + (\"Male\" if gender == 1 else \"Female\")\n                group_total[group] += 1\n                if pred == label:\n                    group_correct[group] += 1\n\n    for group in group_correct:\n        correct = group_correct[group]\n        total = group_total[group]\n        acc = 100 * correct / max(1, total)\n        print(f\"{group}: Accuracy = {acc:.2f}% ({correct}/{total})\")\n\n    overall_acc = 100 * sum(group_correct.values()) / max(1, sum(group_total.values()))\n    print(f\"\\nOverall Accuracy = {overall_acc:.2f}%\")\n    return overall_acc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T17:00:01.778958Z","iopub.execute_input":"2025-07-27T17:00:01.779270Z","iopub.status.idle":"2025-07-27T17:00:01.806777Z","shell.execute_reply.started":"2025-07-27T17:00:01.779247Z","shell.execute_reply":"2025-07-27T17:00:01.805957Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"evaluate_groupwise(model, val_loader, device='cuda')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T17:00:01.808117Z","iopub.execute_input":"2025-07-27T17:00:01.808324Z","iopub.status.idle":"2025-07-27T17:03:16.119759Z","shell.execute_reply.started":"2025-07-27T17:00:01.808309Z","shell.execute_reply":"2025-07-27T17:03:16.119213Z"}},"outputs":[{"name":"stderr","text":"                                                             ","output_type":"stream"},{"name":"stdout","text":"Blond Male: Accuracy = 89.56% (163/182)\nBlond Female: Accuracy = 84.31% (2423/2874)\nNon-Blond Male: Accuracy = 51.22% (4239/8276)\nNon-Blond Female: Accuracy = 82.78% (7065/8535)\n\nOverall Accuracy = 69.91%\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"69.91493431318267"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"torch.save(model.state_dict(), \"resnet18_blond_classifier_domain_adapted.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T17:03:16.120454Z","iopub.execute_input":"2025-07-27T17:03:16.120675Z","iopub.status.idle":"2025-07-27T17:03:16.195221Z","shell.execute_reply.started":"2025-07-27T17:03:16.120651Z","shell.execute_reply":"2025-07-27T17:03:16.194551Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"evaluate_groupwise(model, test_loader, device='cuda')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T17:03:16.195962Z","iopub.execute_input":"2025-07-27T17:03:16.196198Z","iopub.status.idle":"2025-07-27T17:04:52.667658Z","shell.execute_reply.started":"2025-07-27T17:03:16.196175Z","shell.execute_reply":"2025-07-27T17:04:52.666890Z"}},"outputs":[{"name":"stderr","text":"                                                             ","output_type":"stream"},{"name":"stdout","text":"Blond Male: Accuracy = 91.67% (165/180)\nBlond Female: Accuracy = 83.27% (2065/2480)\nNon-Blond Male: Accuracy = 51.61% (3889/7535)\nNon-Blond Female: Accuracy = 85.06% (8308/9767)\n\nOverall Accuracy = 72.27%\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"72.27231740306583"},"metadata":{}}],"execution_count":16}]}